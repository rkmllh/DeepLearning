{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**MLP FROM SCRATCH**\n",
        "\n",
        "This project implements a multi-layer perceptron (MLP) from scratch using numpy, and applies it to classify handwritten digits from the MNIST dataset. The network consists of one hidden layer with ReLU activation and an output layer with softmax activation, because we have 10 outcomes. The notebook covers the forward and backward (2 steps) propagation steps, gradient computation, loss calculation (cross entropy), and parameter updates using stochastic gradient descent."
      ],
      "metadata": {
        "id": "IvWXklofPLiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "y0cd89KFrAH3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Apply the ReLU activation function.\n",
        "\n",
        "    Args:\n",
        "    x -- NumPy array\n",
        "\n",
        "    Returns:\n",
        "    NumPy array with ReLU applied element-wise.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def der_relu(x):\n",
        "    \"\"\"\n",
        "    Compute the derivative of the ReLU function.\n",
        "\n",
        "    Args:\n",
        "    x -- NumPy array\n",
        "\n",
        "    Returns:\n",
        "    NumPy array with the derivative of ReLU applied element-wise.\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Apply the softmax function to a 2D array.\n",
        "\n",
        "    Args:\n",
        "    x -- 2D NumPy array (batch, features)\n",
        "\n",
        "    Returns:\n",
        "    NumPy array with softmax applied along the rows.\n",
        "    \"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "Ym4ZVyTsrKT0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In forward pass, we need to store output of each layer of NN\n",
        "# Please note that we need to start from first layer\n",
        "\n",
        "def forward(X, W0, b0, W1, b1, a=relu):\n",
        "\n",
        "    # f_i represents the pre-activations at the k-th hidden layer\n",
        "    # h_i contains the activations at the k-th hidden layer\n",
        "\n",
        "    # Output of first layer\n",
        "    f0 = b0 + np.dot(X, W0)\n",
        "    # Activation of first layer\n",
        "    h1 = a(f0)\n",
        "\n",
        "    # Final output activated\n",
        "    f1 = b1 + np.dot(h1, W1)\n",
        "    h2 = softmax(f1)\n",
        "\n",
        "    store = (f0, h1, f1, h2)\n",
        "    return h2, store"
      ],
      "metadata": {
        "id": "dMBA6OIPcAeA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to compute derivate of input of our softmax function. Since the steps are a bit delicate, let's write the reasoning that justifies the derivative with respect to the input of the softmax function."
      ],
      "metadata": {
        "id": "1JCY7pvcozrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\frac{\\partial L}{\\partial o_i} = -\\sum_k y_k \\frac{\\partial \\log p_k}{\\partial o_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -\\sum_k \\frac{y_k}{p_k} \\frac{\\partial p_k}{\\partial o_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -y_i (1 - p_i) + \\sum_{k \\neq i} y_k p_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "= p_i - y_i\n",
        "$$\n",
        "\n",
        "where$$o_i$$is input of softmax."
      ],
      "metadata": {
        "id": "GvQk0D_WohBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since all the details are provided in *Understanding Deep Learning, Prince,* let us rewrite the steps for calculating the derivatives of weights and biases.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell_i}{\\partial \\beta_k} = \\frac{\\partial f_k}{\\partial \\beta_k} \\frac{\\partial \\ell_i}{\\partial f_k}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{\\partial}{\\partial \\beta_k} (\\beta_k + \\Omega_k h_k) \\frac{\\partial \\ell_i}{\\partial f_k}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{\\partial \\ell_i}{\\partial f_k},\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell_i}{\\partial \\Omega_k} = \\frac{\\partial f_k}{\\partial \\Omega_k} \\frac{\\partial \\ell_i}{\\partial f_k}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{\\partial}{\\partial \\Omega_k} (\\beta_k + \\Omega_k h_k) \\frac{\\partial \\ell_i}{\\partial f_k}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{\\partial \\ell_i}{\\partial f_k} h_k^\\top.\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\beta_k$ is bias vector in $k$-th layer.\n",
        "- $\\Omega_k$ is weights matrix $k$-th layer.\n",
        "\n",
        "Please note that we have avoided calculating the derivatives of the intermediate levels (details on book).\n"
      ],
      "metadata": {
        "id": "qV-U1Tn11C--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the first step in backward\n",
        "# Here, we compute derivates of other values, not parameters\n",
        "def backward_1(X, y, forward_output, W2):\n",
        "\n",
        "    f0, h1, f1, h2 = forward_output\n",
        "    m = y.shape[0]\n",
        "\n",
        "    # Gradient of input of softmax\n",
        "    df1 = h2 - y\n",
        "\n",
        "    dh1 = np.dot(df1, W2.T)\n",
        "    df0 = dh1 * der_relu(f0)\n",
        "\n",
        "    return df0, df1\n",
        "\n",
        "# This is the second step in backward\n",
        "# Here, we compute derivates of parameters\n",
        "# We prefer normalize the gradient (avoid gradients too unbalanced)\n",
        "def backward_2(X, y, forward_output, W2, df0, df1):\n",
        "    f0, h1, f1, h2 = forward_output\n",
        "    m = y.shape[0]\n",
        "\n",
        "    dW2 = np.dot(h1.T, df1) / m\n",
        "    db2 = np.sum(df1, axis=0, keepdims=True) / m\n",
        "\n",
        "    dW1 = np.dot(X.T, df0) / m\n",
        "    db1 = np.sum(df0, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def backward(X, y, forward_output, W2):\n",
        "\n",
        "    # Please note that these are the only derivates we need\n",
        "    df0, df1 = backward_1(X, y, forward_output, W2)\n",
        "    dW1, db1, dW2, db2 = backward_2(X, y, forward_output, W2, df0, df1)\n",
        "\n",
        "    return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "i1L6UN1GgxZh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    X_train = X_train.reshape(-1, 28*28) / 255.0\n",
        "    X_test = X_test.reshape(-1, 28*28) / 255.0\n",
        "    one_hot = OneHotEncoder(sparse_output=False)\n",
        "    y_train = one_hot.fit_transform(y_train.reshape(-1, 1))\n",
        "    y_test = one_hot.transform(y_test.reshape(-1, 1))\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "Dhh5u33U4rpO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init(input_size, hidden_size, output_size):\n",
        "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "    b1 = np.zeros((1, hidden_size))\n",
        "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "    b2 = np.zeros((1, output_size))\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "xM83G4jf4vJF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_true, y_pred): # Cross entropy\n",
        "    offset = 1e-16\n",
        "    global_loss = np.sum(-np.sum(y_true * np.log(y_pred + offset), axis=1))\n",
        "    mean_loss = np.mean(-np.sum(y_true * np.log(y_pred + offset), axis=1))\n",
        "    return global_loss, mean_loss"
      ],
      "metadata": {
        "id": "PJVc9lq54ycR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "TAgOL5Nu4zco"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, y, hidden_size, output_size, epochs, learning_rate, batch_size, epoch_decay_interval, decay_factor=0.05):\n",
        "    input_size = X.shape[1]\n",
        "    W1, b1, W2, b2 = init(input_size, hidden_size, output_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        if epoch % epoch_decay_interval == 0:\n",
        "            learning_rate -= decay_factor\n",
        "\n",
        "        indices = np.arange(X.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        X, y = X[indices], y[indices]\n",
        "\n",
        "        for i in range(0, X.shape[0], batch_size):\n",
        "            current_x = X[i:i+batch_size]\n",
        "            current_y = y[i:i+batch_size]\n",
        "\n",
        "            output, store = forward(current_x, W1, b1, W2, b2)\n",
        "            dW1, db1, dW2, db2 = backward(current_x, current_y, store, W2)\n",
        "            W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
        "\n",
        "        output, _ = forward(X, W1, b1, W2, b2)\n",
        "        global_loss, mean_loss = loss(y, output)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Global Loss: {global_loss:.4f}, Mean Loss : {mean_loss}\")\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    output, _ = forward(X, W1, b1, W2, b2)\n",
        "    return np.argmax(output, axis=1)\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "hidden_size = 7\n",
        "n_classes = 10\n",
        "\n",
        "W1, b1, W2, b2 = train(X_train, y_train, hidden_size, n_classes, learning_rate=0.2, epochs=10, batch_size=64, epoch_decay_interval=5)\n",
        "\n",
        "y_pred = predict(X_test, W1, b1, W2, b2)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGKUM8JeOU71",
        "outputId": "d18ee03e-974f-4008-8449-007b81bc7e32"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Global Loss: 20632.4497, Mean Loss : 0.34387416169125556\n",
            "Epoch 2/10, Global Loss: 27656.2076, Mean Loss : 0.46093679323616255\n",
            "Epoch 3/10, Global Loss: 18811.3338, Mean Loss : 0.3135222294024702\n",
            "Epoch 4/10, Global Loss: 17459.2583, Mean Loss : 0.2909876377065197\n",
            "Epoch 5/10, Global Loss: 17540.7123, Mean Loss : 0.2923452048932402\n",
            "Epoch 6/10, Global Loss: 17145.9591, Mean Loss : 0.2857659846937174\n",
            "Epoch 7/10, Global Loss: 17955.3903, Mean Loss : 0.2992565042430903\n",
            "Epoch 8/10, Global Loss: 17785.6295, Mean Loss : 0.29642715842418155\n",
            "Epoch 9/10, Global Loss: 16516.7506, Mean Loss : 0.275279176364328\n",
            "Epoch 10/10, Global Loss: 16921.9953, Mean Loss : 0.2820332556441828\n",
            "Test Accuracy: 0.9139\n"
          ]
        }
      ]
    }
  ]
}